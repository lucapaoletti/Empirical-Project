{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.21 in c:\\users\\luca\\appdata\\roaming\\python\\python38\\site-packages (1.21.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install numpy==1.21 --user --> in order to use textProcessing from preProcessing package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import library \n",
    "\n",
    "#from preProcessing import textProcessing\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "import tweepy as tw\n",
    "import credentials\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "\n",
    "from tqdm import notebook\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Classifiers\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "## prediction score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "## Clusterizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMER_KEY = ''\n",
    "CONSUMER_SECRET = ''\n",
    "ACCESS_TOKEN = ''\n",
    "ACCESS_TOKEN_SECRET = ''\n",
    "BEARER_TOKEN = '' # only one needed in the bearer_oauth() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search url for last 7days\n",
    "#search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "\n",
    "#search url for full archive\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "\n",
    "# Optional params: start_time,end_time,since_id,until_id,max_results,next_token,\n",
    "# expansions,tweet.fields,media.fields,poll.fields,place.fields,user.fields\n",
    "#query_params = {'query': '(from:AtheistRepublic -is:retweet)'}\n",
    "\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {BEARER_TOKEN}\"\n",
    "    #r.headers[\"User-Agent\"] = \"v2RecentSearchPython\"\n",
    "    r.headers[\"User-Agent\"] = 'v2FullArchiveSearchPython'\n",
    "    return r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tweet from \n",
    "\n",
    "authors = []\n",
    "texts = []\n",
    "str1 = '(from:'\n",
    "#str2 = ' -is:retweet)'\n",
    "str2 = ' lang:en)'\n",
    "for account in accounts: \n",
    "    request = requests.get(search_url, auth = bearer_oauth, params={'query': str1 + str(account) + str2})\n",
    "    #print(request.json())\n",
    "    try:\n",
    "        for tweet in (request.json()['data']):\n",
    "            authors.append(account)\n",
    "            texts.append(tweet['text'])\n",
    "            \n",
    "    except KeyError: #KeyError quando non ci sono tweet in quel account (possibile se prendo last week come intervallo)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creo df con author e testo del tweet\n",
    "df_data = {'Authors':authors, 'Tweet':texts}\n",
    "df_corpus = pd.DataFrame(df_data)\n",
    "len(df_corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
