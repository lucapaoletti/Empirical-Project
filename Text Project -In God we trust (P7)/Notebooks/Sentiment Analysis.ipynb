{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import re\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "#nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pre-processed data\n",
    "df = pd.read_pickle('path/to/pre-processed.pkl')\n",
    "df = df[[ 'Tweet', 'tokens_no_stop']]#['Authors',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Processing\n",
    "# Remove links\n",
    "def remove_links(tweet):\n",
    "    tweet_no_link = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    return tweet_no_link\n",
    "df['tweet_text_p'] = np.vectorize(remove_links)(df['Tweet'])\n",
    "def remove_links(tweet):\n",
    "    tweet_no_link = re.sub(r\"twitter.com\\S+\", \"\", tweet)\n",
    "    return tweet_no_link\n",
    "df['tweet_text_p'] = np.vectorize(remove_links)(df['tweet_text_p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Twitter Handlers (@Users)\n",
    "def remove_users(tweet, pattern1, pattern2):\n",
    "    r = re.findall(pattern1, tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "    \n",
    "    r = re.findall(pattern2, tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "    return tweet\n",
    "df['tweet_text_p'] = np.vectorize(remove_users)(df['tweet_text_p'], \"@ [\\w]*\", \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Hashtag Symbol\n",
    "# We decided to keep hashtags because they add value to the sentiment.\n",
    "def remove_hashtags(tweet, pattern1):\n",
    "    r = re.findall(pattern1, tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "    return tweet\n",
    "df['tweet_text_p'] = np.vectorize(remove_hashtags)(df['tweet_text_p'], \"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not Remove Punctuation\n",
    "\n",
    "# Removing Punctuation has no significant impact in most cases \n",
    "# In some cases reduces the neutrality because of words in parentheses\n",
    "# df['pre_processed_1'] = df['pre_processed'].str.replace(\"[^a-zA-Z#]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplicates\n",
    "df.drop_duplicates(subset=['tweet_text_p'], keep='first', inplace=True)\n",
    "df = df[~df.tweet_text_p.str.contains(\"Retweeted\")]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object of Vader Sentiment Analyzer\n",
    "vader_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = []\n",
    "neutral = []\n",
    "positive = []\n",
    "compound = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores(df, negative, neutral, positive, compound):\n",
    "    for i in df['tweet_text_p']:\n",
    "        sentiment_dict = vader_analyzer.polarity_scores(i)\n",
    "        negative.append(sentiment_dict['neg'])\n",
    "        neutral.append(sentiment_dict['neu'])\n",
    "        positive.append(sentiment_dict['pos'])\n",
    "        compound.append(sentiment_dict['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calling \n",
    "sentiment_scores(df, negative, neutral, positive, compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare columns to add the scores later\n",
    "df[\"negative\"] = negative\n",
    "df[\"neutral\"] = neutral\n",
    "df[\"positive\"] = positive\n",
    "df[\"compound\"] = compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the overall sentiment with encoding:\n",
    "# (-1)Negative, (0)Neutral, (1)Positive\n",
    "sentiment = []\n",
    "for i in df['compound']:\n",
    "    if i >= 0.05 : \n",
    "        sentiment.append(1)\n",
    "  \n",
    "    elif i <= - 0.05 : \n",
    "        sentiment.append(-1) \n",
    "        \n",
    "    else : \n",
    "        sentiment.append(0)\n",
    "df['sentiment'] = sentiment\n",
    "neg_tweets = df.sentiment.value_counts()[-1]\n",
    "neu_tweets = df.sentiment.value_counts()[0]\n",
    "pos_tweets = df.sentiment.value_counts()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sentiment df\n",
    "df.to_pickle('path/to/sentiment.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tweets by sentiment\n",
    "# Pie Chart\n",
    "\n",
    "# Draw Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6), subplot_kw=dict(aspect=\"equal\"), dpi= 80)\n",
    "\n",
    "data = [df.sentiment.value_counts()[-1], df.sentiment.value_counts()[0], df.sentiment.value_counts()[1]]\n",
    "categories = ['Negative', 'Neutral', 'Positive']\n",
    "explode = [0.05,0.05,0.05]\n",
    "\n",
    "def func(pct, allvals):\n",
    "    absolute = int(pct/100.*np.sum(allvals))\n",
    "    return \"{:.1f}% ({:d} )\".format(pct, absolute)\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(data, \n",
    "                                  autopct=lambda pct: func(pct, data),\n",
    "                                  textprops=dict(color=\"w\"), \n",
    "                                  colors=['#e55039', '#3c6382', '#78e08f'],\n",
    "                                  startangle=140,\n",
    "                                  explode=explode)\n",
    "\n",
    "# Decoration\n",
    "ax.legend(wedges, categories, title=\"Sentiment\", loc=\"center left\", bbox_to_anchor=(1, 0.2, 0.5, 1))\n",
    "plt.setp(autotexts, size=10, weight=700)\n",
    "ax.set_title(\"Number of Tweets by Sentiment\", fontsize=12, fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "freq = [df.sentiment.value_counts()[-1], df.sentiment.value_counts()[0], df.sentiment.value_counts()[1]]\n",
    "index = np.arange(len(freq))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.bar(index, freq, alpha=0.8, color= 'black')\n",
    "plt.xlabel('Sentiment', fontsize=13)\n",
    "plt.ylabel('Number of Tweets', fontsize=13)\n",
    "plt.xticks(index, labels, fontsize=11, fontweight=\"bold\") \n",
    "plt.title('Number of Tweets per Sentiment', fontsize=12, fontweight=\"bold\")\n",
    "plt.ylim(0, len(df['Tweet']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density Plot of Overall compound score\n",
    "\n",
    "# We remove the neutral compound scores to compare the negative and positive tweets\n",
    "data = df[(df[\"sentiment\"]!=0)]\n",
    "# Draw Plot\n",
    "plt.figure(figsize=(8,6), dpi= 80)\n",
    "sns.kdeplot(data[\"compound\"], shade=True, color=\"#3c6382\", label=\"Overall Compound Score\", alpha=.7)\n",
    "\n",
    "# Decoration\n",
    "plt.title('Density Plot of Overall Compound Score', fontsize=11, fontweight='bold')\n",
    "plt.axvline(x=0, color='#e55039')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud of Positive and Negative Tweets\n",
    "# Lemmatization\n",
    "\n",
    "def lemmatization(tweets, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    tweets_out = []\n",
    "    for sent in tweets:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        tweets_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return tweets_out\n",
    "\n",
    "    \n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "df['lemmatized'] = lemmatization(df['tokens_no_stop'], allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "df.drop(['tokens_no_stop'], axis=1, inplace=True)\n",
    "df_pos = df[df['sentiment']==1]\n",
    "df_neg = df[df['sentiment']==(-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Frequency\n",
    "\n",
    "# Join the tweet back together\n",
    "def rejoin_words(row):\n",
    "    words = row['lemmatized']\n",
    "    joined_words = (\" \".join(words))\n",
    "    return joined_words\n",
    "    \n",
    "df_neg['no_stop_joined'] = df_neg.apply(rejoin_words, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = pd.Series(np.concatenate([x.split() for x in df_neg.no_stop_joined])).value_counts()\n",
    "word_df = pd.Series.to_frame(word_freq)\n",
    "word_df['word'] = list(word_df.index)\n",
    "word_df.reset_index(drop=True, inplace=True)\n",
    "word_df.columns = ['freq', 'word']\n",
    "word_df.drop([ 0, 1], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = word_df['word'].head(20)\n",
    "freq = word_df['freq'].head(20)\n",
    "index = np.arange(len(freq))\n",
    "\n",
    "print(\"Unique words:\", len(word_df))\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.bar(index, freq, alpha=0.8, color= 'black')\n",
    "plt.xlabel('Words', fontsize=13)\n",
    "plt.ylabel('Frequency', fontsize=13)\n",
    "plt.xticks(index, label, fontsize=11, rotation=90, fontweight=\"bold\") \n",
    "plt.title('Top 20 Words of Negative tweets after preprocessing', fontsize=12, fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive Tweets\n",
    "# Join the tweet back together\n",
    "def rejoin_words(row):\n",
    "    words = row['lemmatized']\n",
    "    joined_words = (\" \".join(words))\n",
    "    return joined_words\n",
    "\n",
    "    \n",
    "df_pos['no_stop_joined'] = df_pos.apply(rejoin_words, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_pos = pd.Series(np.concatenate([x.split() for x in df_pos.no_stop_joined])).value_counts()\n",
    "word_df_pos = pd.Series.to_frame(word_freq_pos)\n",
    "word_df_pos['word'] = list(word_df_pos.index)\n",
    "word_df_pos.reset_index(drop=True, inplace=True)\n",
    "word_df_pos.columns = ['freq', 'word']\n",
    "word_df_pos.drop([0,2], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = word_df_pos['word'].head(20)\n",
    "freq = word_df_pos['freq'].head(20)\n",
    "index = np.arange(len(freq))\n",
    "\n",
    "print(\"Unique words:\", len(word_df_pos))\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.bar(index, freq, alpha=0.8, color= 'black')\n",
    "plt.xlabel('Words', fontsize=13)\n",
    "plt.ylabel('Frequency', fontsize=13)\n",
    "plt.xticks(index, label, fontsize=11, rotation=90, fontweight=\"bold\") \n",
    "plt.title('Top 20 Words of Positive tweets after preprocessing', fontsize=12, fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud\n",
    "\n",
    "# Join the tweet back together\n",
    "def rejoin_words(row):\n",
    "    words = row['lemmatized']\n",
    "    joined_words = (\" \".join(words))\n",
    "    return joined_words\n",
    "\n",
    "all_words_pos = ' '.join([text for text in df_pos.apply(rejoin_words, axis=1)])\n",
    "all_words_neg = ' '.join([text for text in df_neg.apply(rejoin_words, axis=1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=900, height=600, random_state=21, max_font_size=110, background_color='black', \n",
    "                      max_words=50,colormap='summer').generate(all_words_pos)\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.title('WordCloud of Positive Tweets', fontsize=14, fontweight=\"bold\")\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "\n",
    "wordcloud = WordCloud(width=900, height=600, random_state=21, max_font_size=110, background_color='black', \n",
    "                      max_words=50,colormap='autumn').generate(all_words_neg)\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.title('WordCloud of Negative Tweets', fontsize=14, fontweight=\"bold\")\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=900, height=600, random_state=21, max_font_size=110, background_color='grey', \n",
    "                      max_words=50,colormap='autumn').generate(all_words_neg)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('WordCloud of Negative Tweets', fontsize=14, fontweight=\"bold\")\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compound Score Distribution\n",
    "\n",
    "x1 = df_pos['compound']\n",
    "x2 = df_neg['compound']\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.suptitle('Compound Score Distribution', fontsize=14, fontweight=\"bold\")\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "sns.distplot(x1, color=\"g\", bins=12, hist_kws={\"alpha\": 0.5,\"rwidth\":0.8})\n",
    "plt.title('Positive Tweets',fontsize=12, fontweight=\"bold\")\n",
    "plt.xlabel(\"Compound Score\", fontsize=12)\n",
    "plt.xlim([0,1])\n",
    "\n",
    "# Chart 2: Derivative Function\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(x2, color=\"r\", bins=15, hist_kws={\"alpha\": 0.5, \"rwidth\":0.8})\n",
    "plt.title(\"Negative Tweets\",fontsize=12, fontweight=\"bold\")\n",
    "plt.xlabel(\"Compound Score\", fontsize=12)\n",
    "plt.xlim([-1,0])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 Most Positive and Negative Tweets\n",
    "# 10 Most positive Tweets\n",
    "df_pos.sort_values('compound', inplace=True, ascending=False)\n",
    "df_pos.reset_index(drop=True, inplace=True)\n",
    "df_pos.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 Most Negative Tweets\n",
    "df_neg.sort_values('compound', inplace=True)\n",
    "df_neg.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_neg.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg.to_csv(\"path/to/df_neg.csv\", sep=';')\n",
    "df_pos.to_csv(\"path/to/df_pos.csv\", sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5e3ca5b78f5017bf8058e475be571c8c3aef9f26ab369f65f483a55f28d9ec3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
