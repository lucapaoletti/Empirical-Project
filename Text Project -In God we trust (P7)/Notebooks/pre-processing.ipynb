{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "# general \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# gensim\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT TWEETS\n",
    "df = pd.read_csv(\"path/to/Tweet_update.csv\", sep=',')\n",
    "df = df[['Authors', 'Tweet']]\n",
    "#df.columns = df.iloc[0]\n",
    "#df = df[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tweets to string\n",
    "df['Tweet'] = df['Tweet'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates (possible when answering to tweet)\n",
    "df.drop_duplicates(subset = ['Tweet'], keep='first', inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count toal number of charctaers and mean length of a tweet\n",
    "count = df['Tweet'].str.split().str.len()\n",
    "count.index = count.index.astype(str) + ' words:'\n",
    "count.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of words:', count.sum(), 'words')\n",
    "print('Mean number of words per tweet:', round(count.mean(), 2), 'words')\n",
    "\n",
    "df['tweet_length'] = df['Tweet'].str.len()\n",
    "print('Total length of dataset is:', df.tweet_length.sum(), 'characters')\n",
    "\n",
    "print('Mean Length of a tweet is:', round(df.tweet_length.mean(), 0), 'characters')\n",
    "df = df.drop(['tweet_length'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Twitter Handles (@user)\n",
    "\n",
    "def remove_users(tweet, pattern1, pattern2):\n",
    "    r = re.findall(pattern1, tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "    r = re.findall(pattern2, tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tidy_tweet'] = np.vectorize(remove_users)(df['Tweet'], '@ [\\w]*', '@[\\w]*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase\n",
    "df['tidy_tweet'] = df['tidy_tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashtag\n",
    "df['hashtags'] = df['tidy_tweet'].apply(lambda twt : re.findall(r\"#(\\w+)\", twt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Counter(df.hashtags.sum())\n",
    "df_hashtags = pd.DataFrame([d]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hashtags.columns = ['freq']\n",
    "df_hashtags.freq.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hashtags.sort_values(by=['freq'], ascending=False, inplace=True)\n",
    "df_hashtags = df_hashtags[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Top Hashtags\n",
    "labels = df_hashtags.head(20).index.values.tolist()\n",
    "freq = df_hashtags['freq'].head(20).values.tolist()\n",
    "index = np.arange(len(freq))\n",
    "print('Among (insert number of tweet), (insert number of hashtags were used.')\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.bar(index, freq, alpha = 0.8, color = 'black')\n",
    "plt.xlabel('Hashtags', fontsize = 13)\n",
    "plt.ylabel('Frequency', fontsize = 13)\n",
    "plt.xticks(index, labels, fontsize = 11, rotation = 90, fontweight = 'bold')\n",
    "plt.title('Top 20 Hashtags of dataset', fontsize = 12, fontweight = 'bold')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Hashtag column\n",
    "df = df.drop(['hashtags'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Hashtags from tweets\n",
    "def remove_hashtags(tweet, pattern1, pattern2):\n",
    "    r = re.findall(pattern1, tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "    \n",
    "    r = re.findall(pattern2, tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tidy_tweet'] = np.vectorize(remove_hashtags)(df['tidy_tweet'], '# [\\w]*', '#[\\w]*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Links\n",
    "def remove_links(tweet):\n",
    "    tweet_no_link = re.sub(r'http\\S+', '', tweet)\n",
    "    return tweet_no_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tidy_tweet'] = np.vectorize(remove_links)(df['tidy_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuations, Numbers, and Special Characters\n",
    "df['tidy_tweet'] = df['tidy_tweet'].str.replace('[^a-zA-Z#]', ' ') # [^a-zA-Z#] --> non letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove short words (the word 'not' is removed, think about it) Tweet is a short text, a negative word is more effective \n",
    "# in this case, imo --> i remove 'NOT' words\n",
    "df['tidy_tweet'] = df['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words and clean-up Punctuations\n",
    "def tokenize(tweet):\n",
    "    for word in tweet:\n",
    "        yield(gensim.utils.simple_preprocess(str(word), deacc=True)) # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tidy_tweet_tokens'] = list(tokenize(df['tidy_tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords and custom stopwords\n",
    "# prepare stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'https', 'twitter', 'pic', 'twitt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tweets):\n",
    "    return [[word for word in simple_preprocess(str(tweet)) if word not in stop_words] for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens_no_stop'] = remove_stopwords(df['tidy_tweet_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows having less than 3 tokens\n",
    "df['length'] = df['tokens_no_stop'].apply(len)\n",
    "df = df.drop(df[df['length']<3].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['length'], axis = 1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop = True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud\n",
    "\n",
    "# join the tweet back together\n",
    "def rejoin_words(row):\n",
    "    words = row['tokens_no_stop']\n",
    "    joined_words = (' '.join(words))\n",
    "    return joined_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_stop_joined'] = df.apply(rejoin_words, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([text for text in df['no_stop_joined']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=900, height=600, random_state=21, max_font_size=110, background_color='ghostwhite', max_words=200, colormap='Dark2').generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning effects - Length of tweets\n",
    "df['tweet_length'] = df['Tweet'].str.len()\n",
    "df['cleaned_tweet_length'] = df['no_stop_joined'].str.len()\n",
    "df_lengths = df[['tweet_length', 'cleaned_tweet_length']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = df_lengths.tweet_length\n",
    "x2 = df_lengths.cleaned_tweet_length\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.suptitle('Length of tweet as number of characters', fontsize = 14, fontweight = 'bold')\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(x1, color='black', label='No. characters', bins=35, hist_kws={'alpha':0.5, 'rwidth':0.8})\n",
    "plt.title('Original Tweets', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Number of characters', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.xlim([0, 400])\n",
    "\n",
    "# Chart 2: Derivative Function\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.distplot(x2, color='black', label='No. characters', bins=17, hist_kws={'alpha':0.5, 'rwidth':0.8})\n",
    "plt.title('Cleaned Tweets', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Number of characters', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.xlim([0, 400])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_words'] = df['Tweet'].str.split().str.len()\n",
    "df['cleaned_tweet_words'] = df['no_stop_joined'].str.split().str.len()\n",
    "df_lengths = df[['tweet_words', 'cleaned_tweet_words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = df_lengths.tweet_words\n",
    "x_2 = df_lengths.cleaned_tweet_words\n",
    "plt.figure(figsize = (15, 6))\n",
    "plt.suptitle('Length of tweet as number of words.', fontsize=14, fontweight='bold')\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "sns.distplot(x_1, color='black', label='No. Words', bins=25, hist_kws={'alpha':0.5, 'rwidth':0.8})\n",
    "plt.title('Original Tweets', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Number of words', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.xlim([0, 70])\n",
    "\n",
    "# Chart 2: Derivative Function\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.distplot(x_2, color='black', label='No. Words', bins=15, hist_kws={'alpha':0.5, 'rwidth':0.8})\n",
    "plt.title('Cleaned Tweets', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Number of words', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.xlim([0, 70])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 25 most frequent words\n",
    "word_freq = pd.Series(np.concatenate([x.split() for x in df.no_stop_joined])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.Series.to_frame(word_freq)\n",
    "word_df['word'] = list(word_df.index)\n",
    "word_df.reset_index(drop=True, inplace=True)\n",
    "word_df.columns = ['freq', 'word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = word_df['word'].head(25)\n",
    "freq = word_df['freq'].head(25)\n",
    "index = np.arange(len(freq))\n",
    "\n",
    "print('Unique words:', len(word_df))\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.bar(index, freq, alpha=0.8, color='black')\n",
    "plt.xlabel('Words', fontsize=13)\n",
    "plt.ylabel('Frequency', fontsize=13)\n",
    "plt.xticks(index, label, fontsize=11, rotation=90, fontweight='bold')\n",
    "plt.title('Top 25 Words after preprocessing', fontsize=12, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df\n",
    "df.to_pickle('path/to/pre-processed.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5e3ca5b78f5017bf8058e475be571c8c3aef9f26ab369f65f483a55f28d9ec3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
